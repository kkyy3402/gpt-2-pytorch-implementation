# config.yaml
project_name: "gpt-2-pretraining"
  
# 데이터 관련 설정
data:
  # url: "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
  dir: "data"
  split: "train"
  name: "maywell/korean_textbooks"
  raw_path: "data/input.txt" 
  processed_path: "data/processed_data.txt"
  subset_name: "claude_evol"

# 토크나이저 관련 설정
tokenizer:
  save_path: "tokenizer.json"
  # vocab_size: 50257  # 어휘 사전 크기

# Base Model 기준
model:
  embed_size: 768
  num_layers: 12
  num_heads: 12
  ff_hidden_dim: 3072
  max_length: 512
  dropout: 0.1
  save_path: "output.pth"

# 학습 관련 설정
training:
  batch_size: 256
  seq_length: 50
  epochs: 20
  learning_rate: 3e-5

# 기타 설정
device:
  use_cuda: true
  use_mps: true