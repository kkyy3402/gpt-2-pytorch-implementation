# config.yaml
project_name: "gpt-2-pretraining"
  
# 데이터 관련 설정
data:
  dir: "data"
  split: "train"
  name: "wikimedia/wikipedia"
  raw_path: "data/input.txt" 
  processed_path: "data/processed_data.txt"
  subset_name: "20231101.ko"

# Base Model 기준
model:
  embed_size: 768
  num_layers: 12
  num_heads: 12
  ff_hidden_dim: 3072
  max_length: 512
  dropout: 0.1
  save_path: "output.pth"

# 학습 관련 설정
training:
  batch_size: 256
  seq_length: 50
  epochs: 20
  learning_rate: 3e-5
  early_stopping_patience: 20

# 기타 설정
device:
  use_cuda: true
  use_mps: true